I worked with the classic Iris dataset from sklearn.datasets, which contains 150 samples from three flower species: Setosa, Versicolor, and Virginica. I selected petal length and width as features, since they provide clear visual separation between classes and require no additional preprocessing.
The main goal of this project was to better understand how different classification algorithms — such as Logistic Regression, Random Forest, and Support Vector Machines — behave when tuning hyperparameters. I focused on visualizing their decision boundaries and learning how model complexity affects overfitting and underfitting.

## Iris Dataset Classifier Exploration

This project explores the well-known Iris dataset (`sklearn.datasets`), which contains 150 samples of three flower species: *Setosa*, *Versicolor*, and *Virginica*. I selected petal length and width as features due to their clear class separation and minimal preprocessing requirements.

The primary goal was to better understand how different classification algorithms — such as Logistic Regression, Random Forest, and Support Vector Machines — behave under different hyperparameter settings. I focused on visualizing their decision boundaries and observing how model complexity impacts overfitting and underfitting.

### Key Concepts:
- Model tuning and interpretation
- Visual comparison of classifiers
- Recognizing signs of overfitting vs underfitting

All experiments were done in a clean, minimal setting to focus on the model behavior itself.
