{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "db7df796-5d3c-436d-9fc2-dc793094b79b",
      "metadata": {
        "id": "db7df796-5d3c-436d-9fc2-dc793094b79b"
      },
      "outputs": [],
      "source": [
        "###model.py\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, regularizers, optimizers\n",
        "import polars as pl\n",
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "@register_keras_serializable()\n",
        "class ColaborativeFiltering(tf.keras.Model):\n",
        "    def __init__(self, num_user_features, num_movie_features, user_layers=[128, 64], movie_layers=[128, 64], embedding=32, learning_rate=0.001, user_reg = None, movie_reg = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_user_features = num_user_features\n",
        "        self.num_movie_features = num_movie_features\n",
        "        self.embedding = embedding\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # User branch\n",
        "        user_dense_layers = []\n",
        "        for i, units in enumerate(user_layers):\n",
        "            reg = user_reg[i] if user_reg is not None else None\n",
        "            user_dense_layers.append(layers.Dense(units, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=reg))\n",
        "\n",
        "        user_dense_layers.append(layers.Dense(self.embedding, activation='tanh', kernel_initializer='glorot_uniform'))\n",
        "        self.user_net = tf.keras.Sequential(user_dense_layers)\n",
        "\n",
        "        # movie branch\n",
        "        movie_dense_layers = []\n",
        "        for units in movie_layers:\n",
        "            movie_dense_layers.append(layers.Dense(units, activation='tanh', kernel_initializer='glorot_uniform'))\n",
        "        movie_dense_layers.append(layers.Dense(self.embedding, activation='tanh', kernel_initializer='glorot_uniform'))\n",
        "        self.movie_net = tf.keras.Sequential(movie_dense_layers)\n",
        "\n",
        "        self.dot = layers.Dot(axes=1, name='cosine_similarity')\n",
        "\n",
        "        # Save architecture parameters for serialization\n",
        "        self.user_layers = user_layers\n",
        "        self.movie_layers = movie_layers\n",
        "        self.user_reg = user_reg\n",
        "        self.movie_reg = movie_reg\n",
        "\n",
        "        self.compile(\n",
        "            optimizer=optimizers.Nadam(learning_rate=self.learning_rate),\n",
        "            loss='mse',\n",
        "            metrics=['mae', 'mse']\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_user_features': self.num_user_features,\n",
        "            'num_movie_features': self.num_movie_features,\n",
        "            'embedding': self.embedding,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'user_layers': self.user_layers,\n",
        "            'movie_layers': self.movie_layers,\n",
        "            'user_reg': self.user_reg,\n",
        "            'movie_reg': self.movie_reg\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        user_input, movie_input = inputs\n",
        "        user_embedding = tf.nn.l2_normalize(self.user_net(user_input), axis=1)\n",
        "        movie_embedding = tf.nn.l2_normalize(self.movie_net(movie_input), axis=1)\n",
        "        cos_sim = self.dot([user_embedding, movie_embedding])\n",
        "        return cos_sim\n",
        "\n",
        "    def recommend(self, user_vec, movie_matrix, user_seen_movie_indices = None, k=10, movie_titles=None):\n",
        "        user_vecs = tf.repeat(tf.reshape(user_vec, (1, -1)), tf.shape(movie_matrix)[0], axis=0)\n",
        "        preds = self.predict([user_vecs, movie_matrix])\n",
        "        # mask_indices = tf.constant(list(user_seen_movie_indices), dtype=tf.int32)\n",
        "        # preds = tf.tensor_scatter_nd_update(\n",
        "        #     tf.squeeze(preds),\n",
        "        #     tf.expand_dims(mask_indices, 1),\n",
        "        #     tf.fill([tf.size(mask_indices)], tf.constant(-float('inf'), dtype=preds.dtype))\n",
        "        # )\n",
        "        top_k_idx = tf.argsort(preds, direction='DESCENDING')[:k]\n",
        "        if movie_titles is not None:\n",
        "            return [(movie_titles[int(i)], float(preds[i])) for i in top_k_idx]\n",
        "        else:\n",
        "            return [(int(i), float(preds[i])) for i in top_k_idx]\n",
        "\n",
        "    def get_user_seen_movie_indices(self, user_id, ratings, movies):\n",
        "        gledani_movieid = set(ratings.filter(pl.col('userid') == user_id)['movieid'].to_list())\n",
        "        movieid_to_idx = {movie_id: idx for idx, movie_id in enumerate(movies['movieid'].to_list())}\n",
        "        return {movieid_to_idx[movie_id] for movie_id in gledani_movieid if movie_id in movieid_to_idx}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###prep.py\n",
        "\n",
        "\n",
        "import polars as pl\n",
        "import os\n",
        "from sqlalchemy import create_engine\n",
        "import tensorflow as tf\n",
        "\n",
        "'''Funkcije za pripremu podataka za collaborative filtering model'''\n",
        "\n",
        "def read_data_lake():\n",
        "    '''\n",
        "    Data lake --> Polars.DataFrame\n",
        "    '''\n",
        "    engine = create_engine(f\"postgresql+psycopg2://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/movie_recommendation\")\n",
        "    conn = engine.connect()\n",
        "    ratings = pl.read_database(query='SELECT * FROM data_lake.ratings', connection=conn)\n",
        "    movies = pl.read_database(query='SELECT * FROM raw.movies', connection=conn)\n",
        "    conn.close()\n",
        "    return ratings, movies\n",
        "\n",
        "def prep_pipeline(ratings, movies, user_id = None):\n",
        "    '''\n",
        "    Priprema za model\n",
        "    '''\n",
        "    #PROSECAN BROJ OCENA PO FILMU\n",
        "    num_ratings = ratings.group_by('movieid').agg(pl.len().alias('#ratings_film'))\n",
        "    user = ratings.join(num_ratings, on = 'movieid', how = 'inner').sort(['movieid', 'userid'])\n",
        "    movies = movies.with_columns(pl.col(\"genres\").str.split(\"|\"))\n",
        "    unique_genres = sorted(set(g for genre in movies[\"genres\"] for g in genre))\n",
        "    #LAZY!\n",
        "    user = user.lazy()\n",
        "    movies = movies.lazy()\n",
        "    #SVI ZANROVI\n",
        "    for genre in unique_genres:\n",
        "        movies = movies.with_columns(pl.col(\"genres\").list.contains(genre).cast(pl.Int8).alias(genre))\n",
        "    movies = movies.drop('genres')\n",
        "    #KOLONA GODINA\n",
        "    movies = movies.with_columns(pl.col(\"title\").str.extract(r\"\\((\\d{4})\\)\", 1).cast(pl.Int16).alias(\"year\"))\n",
        "\n",
        "    #ISTI FORMAT TABELE KAO MOVIES\n",
        "    user_zanr_train = user.join(movies, on='movieid', how='inner')\n",
        "\n",
        "    #PIVOT LONGER --> ZANROVE PREBACUJEM U JEDNU KOLONU\n",
        "    user_longer = (user_zanr_train.unpivot(index=['userid', 'rating'],\n",
        "                                           on=unique_genres).filter(pl.col('value') == 1).rename({'variable': 'genre', 'value': 'is_genre'}))\n",
        "\n",
        "    #RACUNAM PROSEK ZA SVAKOG USERA ZA SVAKI ZANR I VRACAM U WIDE FORMAT\n",
        "    user_feature = user_longer.group_by('userid').agg([(pl.when(pl.col('genre') == genre).then(pl.col('rating')).mean().alias(genre)) for genre in unique_genres]).fill_null(0)\n",
        "    movie_avg_rating = (user.group_by('movieid').agg(pl.col('rating').mean().alias('avg_rating')))\n",
        "    movie_features = movies.join(movie_avg_rating, on='movieid', how='left').fill_null(0)\n",
        "    movie_features = movie_features.select(['movieid', 'title','year','avg_rating', *unique_genres])\n",
        "    df = user.join(user_feature, on=\"userid\", how=\"inner\").join(movie_features, on=\"movieid\", how=\"inner\")\n",
        "    df = df.collect()\n",
        "    movie_features = movie_features.rename({\"(no genres listed)\": \"no genres listed\"})\n",
        "    user_feature = user_feature.rename({\"(no genres listed)\": \"no genres listed\"})\n",
        "    df = df.rename({\"(no genres listed)\": \"no genres listed\"})\n",
        "    user_feature = user_feature.sort('userid')\n",
        "    df = df.sort('userid')\n",
        "\n",
        "    return user_feature.collect(), movie_features.collect(), df\n",
        "\n",
        "\n",
        "def global_scalers():\n",
        "    engine = create_engine(f\"postgresql+psycopg2://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/movie_recommendation\")\n",
        "    conn = engine.connect()\n",
        "    df = pl.read_database(query='SELECT * FROM raw.ratings', connection=conn)\n",
        "    user, movies_feat, df = prep_pipeline(df, pl.read_database(query='SELECT * FROM raw.movies', connection=conn))\n",
        "    _, _ , _, scalers = scale(df, user, movies_feat)\n",
        "    conn.close()\n",
        "    return scalers\n",
        "\n",
        "def scale(df, user, movies, user_id = None):\n",
        "    '''\n",
        "    Skaliranje numeriƒçkih karakteristika i prebacivanje u tenzore\n",
        "    df - Polars DataFrame sa svim podacima\n",
        "    user - Polars DataFrame sa korisniƒçkim karakteristikama\n",
        "    movies - Polars DataFrame sa filmskim karakteristikama\n",
        "    user_id - ako je None, onda se vracaju svi korisnici, ako je lista (ili int) onda se vraca samo taj korisnik\n",
        "\n",
        "    '''\n",
        "    y = tf.convert_to_tensor(df.select(pl.col('rating')).to_numpy(), dtype=tf.float16)\n",
        "\n",
        "    prva_user = df.columns.index('no genres listed')\n",
        "    poslednja_user = df.columns.index('Western')\n",
        "    ###prva kolona u X_user_ud je userid!!!, trebace za preporuke, za treniranje koristiti X_user\n",
        "    X_user_id = tf.convert_to_tensor(df.select(['userid'] + df.columns[prva_user : poslednja_user + 1]).to_numpy(), dtype=tf.float32)\n",
        "    X_movie_df = df.select(['year','avg_rating', '#ratings_film'] + [col for col in df.columns if col.endswith('_right')])\n",
        "    movie_num = tf.convert_to_tensor(X_movie_df.select(['#ratings_film', 'year', 'avg_rating']).to_numpy(), dtype=tf.float32)\n",
        "    movie_cat = tf.convert_to_tensor(X_movie_df.select(pl.all().exclude(['#ratings_film', 'year', 'avg_rating'])).to_numpy(), dtype=tf.float32)\n",
        "    # Standardizacija user i movie numeriƒçkih\n",
        "    X_user = X_user_id[:, 1:]\n",
        "    user_mean = tf.reduce_mean(X_user, axis=0)\n",
        "    user_std = tf.math.reduce_std(X_user, axis=0)\n",
        "    X_user_scaled = (X_user - user_mean) / (user_std+ 1e-8)\n",
        "    X_user_id_scaled = tf.concat([X_user_id[:, :1], X_user_scaled], axis=1)  # Skalirano sa ID kolonom\n",
        "    movie_mean = tf.reduce_mean(movie_num, axis=0)\n",
        "    movie_std = tf.math.reduce_std(movie_num, axis=0)\n",
        "    movie_num_scaled = (movie_num - movie_mean) / (movie_std)\n",
        "    X_movie_scaled = tf.concat([movie_num_scaled, movie_cat], axis=1)\n",
        "    # Target skaliranje na [-1, 1]\n",
        "    y_scaled = 2 * (y - tf.reduce_min(y)) / (tf.reduce_max(y) - tf.reduce_min(y)) - 1\n",
        "    scalers = {\"user_mean\": user_mean, \"user_std\": user_std,\"movie_mean\": movie_mean,\"movie_std\": movie_std, \"y_min\": tf.reduce_min(y), \"y_max\": tf.reduce_max(y)}\n",
        "    if user_id is not None:\n",
        "        ###Ako je dat user id, filtriramo X_user_id_scaled i X_movie_scaled i vracamo samo korisnika sa tim user_id-om, ako nije vracamo sve korisnike\n",
        "        maska = tf.reduce_any(tf.equal(tf.expand_dims(X_user_id_scaled[:, 0], 1), tf.constant(user_id, dtype=X_user_id_scaled.dtype)), axis=1)\n",
        "        X_user_id_scaled = tf.boolean_mask(X_user_id_scaled, maska)\n",
        "        X_movie_scaled = tf.boolean_mask(X_movie_scaled, maska)\n",
        "        y_scaled = tf.boolean_mask(y_scaled, maska)\n",
        "        return X_user_id_scaled, X_movie_scaled, y_scaled, scalers\n",
        "    # Ako user_id nije naveden, vracamo sve korisnike bez filtriranja user_id-a\n",
        "    else:\n",
        "        return X_user_scaled, X_movie_scaled, y_scaled, scalers\n",
        "\n",
        "\n",
        "def batch_generator(movies, batch_size=1000000, total = 2e7):\n",
        "    '''\n",
        "    Pravi skupove od batch_size (milion) iz nasumicnih total (20 miliona) redova u tabeli ratings\n",
        "    '''\n",
        "    engine = create_engine(f\"postgresql+psycopg2://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/movie_recommendation\")\n",
        "    conn = engine.connect()\n",
        "    offset = 0\n",
        "    while offset < total:\n",
        "        query = f\"SELECT * FROM raw.ratings LIMIT {batch_size} OFFSET {offset}\"\n",
        "        batch = pl.read_database(query=query, connection=conn)\n",
        "        if batch.height == 0:\n",
        "            break\n",
        "        user, movies_feat, df = prep_pipeline(batch, movies, batch)\n",
        "        X_user, X_movie, y, scalers = scale(df, user, movies_feat)\n",
        "        yield (X_user, X_movie), tf.squeeze(y)\n",
        "        offset += batch_size\n",
        "    conn.close()\n",
        "\n",
        "# def train_test_split(X_user, X_movie, y, test_size=0.2, random_state= 42):\n",
        "#     N = X_user.shape[0]\n",
        "#     tf.random.set_seed(random_state)\n",
        "#     idx = tf.random.shuffle(tf.range(N))\n",
        "#     split = int(N * (1 - test_size))\n",
        "#     train_idx = idx[:split]\n",
        "#     dev_idx = idx[split:]\n",
        "\n",
        "#     X_user_train, X_movie_train, y_train = tf.gather(X_user, train_idx), tf.gather(X_movie, train_idx), tf.gather(y, train_idx)\n",
        "\n",
        "#     X_user_dev, X_movie_dev, y_dev = tf.gather(X_user, dev_idx), tf.gather(X_movie, dev_idx), tf.gather(y, dev_idx)\n",
        "\n",
        "#     return (X_user_train, X_movie_train), y_train, (X_user_dev, X_movie_dev), y_dev\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def NN_prep(df, user, movies, user_id = None):\n",
        "#     '''\n",
        "#     Prebacivanje u tenzore i skaliranje --> tf.Tensor\n",
        "#     user_id - za listu usera, ako je None onda vraca tf.Tensor sa svim userima\n",
        "#     '''\n",
        "#     y = tf.convert_to_tensor(df.select(pl.col('rating')).to_series().to_list(), dtype=tf.float32)\n",
        "#     prva_user = df.columns.index('no genres listed')\n",
        "#     poslednja_user = df.columns.index('Western')\n",
        "#     if user_id is None:\n",
        "#         X_user = tf.convert_to_tensor(df.select(df.columns[prva_user : poslednja_user + 1])\n",
        "# .to_numpy(), dtype=tf.float32)\n",
        "#     else:\n",
        "#         X_user = tf.convert_to_tensor(df.filter(pl.col('userid') == user_id).select(df.columns[prva_user : poslednja_user + 1]).to_numpy(), dtype=tf.float32)\n",
        "#     X_movie_df = df.select(['year','avg_rating', '#ratings_film'] + [col for col in df.columns if col.endswith('_right')])\n",
        "#     movie_num = tf.convert_to_tensor(X_movie_df.select(['#ratings_film', 'year', 'avg_rating']).to_numpy(), dtype=tf.float32)\n",
        "#     movie_cat = tf.convert_to_tensor(X_movie_df.select(pl.all().exclude(['#ratings_film', 'year', 'avg_rating'])).to_numpy(), dtype=tf.float32)\n",
        "#     # Skaliranje (standardizacija) user i movie numeriƒçkih karakteristika\n",
        "#     user_mean = tf.reduce_mean(X_user, axis=0)\n",
        "#     user_std = tf.math.reduce_std(X_user, axis=0)\n",
        "#     X_user_scaled = (X_user - user_mean) / (user_std + 1e-7)\n",
        "#     movie_mean = tf.reduce_mean(movie_num, axis=0)\n",
        "#     movie_std = tf.math.reduce_std(movie_num, axis=0)\n",
        "#     movie_num_scaled = (movie_num - movie_mean) / (movie_std)\n",
        "#     X_movie_scaled = tf.concat([movie_num_scaled, movie_cat], axis=1)\n",
        "#     # Target skaliranje na [-1, 1]\n",
        "#     y_min = tf.reduce_min(y)\n",
        "#     y_max = tf.reduce_max(y)\n",
        "#     y_scaled = 2 * (y - y_min) / (y_max - y_min) - 1\n",
        "\n",
        "#     # Vrati i transformatore za kasniju upotrebu\n",
        "#     scalers = {\"user_mean\": user_mean, \"user_std\": user_std, \"movie_mean\": movie_mean, \"movie_std\": movie_std, \"y_min\": y_min, \"y_max\": y_max}\n",
        "\n",
        "#     return X_user_scaled, X_movie_scaled, y_scaled, scalers\n",
        "\n",
        "def inverse_transform_y(y_scaled, scalers):\n",
        "    \"\"\"\n",
        "    Inverzna transformacija za y skaliran na [-1, 1].\n",
        "    \"\"\"\n",
        "    y_min = scalers[\"y_min\"]\n",
        "    y_max = scalers[\"y_max\"]\n",
        "    y = (y_scaled + 1) * (y_max - y_min) / 2 + y_min\n",
        "    return y\n",
        "\n",
        "def inverse_transform_X_user(X_user_scaled, scalers):\n",
        "    \"\"\"\n",
        "    Inverzna transformacija za X_user.\n",
        "    \"\"\"\n",
        "    user_mean = scalers[\"user_mean\"]\n",
        "    user_std = scalers[\"user_std\"]\n",
        "    return X_user_scaled * (user_std + 1e-8) + user_mean\n",
        "\n",
        "def inverse_transform_X_movie_num(X_movie_num_scaled, scalers):\n",
        "    \"\"\"\n",
        "    Inverzna transformacija za numeriƒçke karakteristike filma.\n",
        "    \"\"\"\n",
        "    movie_mean = scalers[\"movie_mean\"]\n",
        "    movie_std = scalers[\"movie_std\"]\n",
        "    return X_movie_num_scaled * (movie_std + 1e-8) + movie_mean\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7NL2TmTQzqQk"
      },
      "id": "7NL2TmTQzqQk",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###train.py\n",
        "\n",
        "# from prep import *\n",
        "# from model import ColaborativeFiltering\n",
        "from sqlalchemy import create_engine\n",
        "import polars as pl\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras import layers, Input, regularizers, Model, optimizers\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import joblib\n",
        "\n",
        "# engine = create_engine(f\"postgresql+psycopg2://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost:5432/movie_recommendation\")\n",
        "# conn = engine.connect()\n",
        "# movies = pl.read_database(query='SELECT * FROM raw.movies', connection=conn)\n",
        "# conn.close()\n",
        "\n",
        "total = 50000\n",
        "\n",
        "data = tf.data.Dataset.from_generator(\n",
        "    lambda: batch_generator(movies, batch_size=4096, total = total),\n",
        "    output_signature=(\n",
        "        (tf.TensorSpec(shape=(None, 20), dtype=tf.float32, name= 'X_user'),\n",
        "         tf.TensorSpec(shape=(None, 23), dtype=tf.float32, name='X_movie')),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.float32, name='y')\n",
        "    )\n",
        ")\n",
        "\n",
        "(X_user_dev, X_movie_dev), y_dev = next(iter(data))\n",
        "training_batch = 4096\n",
        "train_data = data.unbatch().batch(training_batch).skip(1).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "\n",
        "# import numpy as np\n",
        "# np.isnan(X_user_dev.numpy()).any()\n",
        "# np.isnan(X_movie_dev.numpy()).any()\n",
        "# np.isnan(y_dev.numpy()).any()\n",
        "\n",
        "\n",
        "\n",
        "# for batch in train_data.take(5):\n",
        "#     (X_user_batch, X_movie_batch), y_batch = batch\n",
        "#     print(\"X_user_batch shape:\", X_user_batch.shape)\n",
        "#     print(\"X_movie_batch shape:\", X_movie_batch.shape)\n",
        "#     print(\"y_batch shape:\", y_batch.shape)\n",
        "#     print(\"X_user_batch:\", X_user_batch.numpy()[0])\n",
        "#     print(\"X_movie_batch:\", X_movie_batch.numpy()[0])\n",
        "#     print(\"y_batch:\", y_batch.numpy()[0])\n",
        "\n",
        "for batch in train_data.take(15):\n",
        "    (X_user_batch, X_movie_batch), y_batch = batch\n",
        "    print(np.isnan(X_user_batch.numpy()).any(), np.isnan(X_movie_batch.numpy()).any(), np.isnan(y_batch.numpy()).any())\n",
        "    print(np.isinf(X_user_batch.numpy()).any(), np.isinf(X_movie_batch.numpy()).any(), np.isinf(y_batch.numpy()).any())\n",
        "    print(\"y_batch min/max:\", y_batch.numpy().min(), y_batch.numpy().max())\n",
        "\n",
        "# for i, batch in enumerate(train_data.take(8)):\n",
        "#     (X_user_batch, X_movie_batch), y_batch = batch\n",
        "#     nan_mask = np.isnan(X_user_batch.numpy()).any(axis=1)\n",
        "#     if nan_mask.any():\n",
        "#         print(f\"Batch {i+1} ima NaN u X_user_batch na indeksima:\", np.where(nan_mask)[0])\n",
        "#         print(\"Redovi sa NaN:\", X_user_batch.numpy()[nan_mask])\n",
        "#         # Opcionalno: pogledaj i y_batch[nan_mask], X_movie_batch[nan_mask]\n",
        "\n",
        "\n",
        "\n",
        "model = ColaborativeFiltering(20, 23 ,user_layers = [256, 128, 64],embedding=64, learning_rate=0.001)#, user_reg = [regularizers.l2(0.01), None, None])\n",
        "model.summary()\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True), ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)]\n",
        "history = model.fit(train_data, epochs=20, validation_data=([X_user_dev,  X_movie_dev], y_dev), callbacks=callbacks, steps_per_epoch = int(total // training_batch))\n",
        "\n",
        "\n",
        "model.save('model_proba.keras')\n",
        "joblib.dump(history, 'history_proba.pkl')\n",
        "joblib.dump(scalers, 'scalers_proba.pkl')\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "# load_model('model_proba.keras')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jBd0F3JdzuYD",
        "outputId": "3b92da1a-6163-42f7-d854-28f45f7aaa7b"
      },
      "id": "jBd0F3JdzuYD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'movies' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-5-a0dea4b0d7c8>\", line 21, in <lambda>\n    lambda: batch_generator(movies, batch_size=4096, total = total),\n                            ^^^^^^\n\nNameError: name 'movies' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a0dea4b0d7c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_user_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_movie_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtraining_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'movies' is not defined\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 865, in get_iterator\n    return self._iterators[iterator_id]\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n\nKeyError: np.int64(0)\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 867, in get_iterator\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-5-a0dea4b0d7c8>\", line 21, in <lambda>\n    lambda: batch_generator(movies, batch_size=4096, total = total),\n                            ^^^^^^\n\nNameError: name 'movies' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###TRAIN.py\n",
        "\n",
        "###\n",
        "#samo za RZS\n",
        "movies = pl.read_csv(r'https://raw.githubusercontent.com/BogdanSliskovic/ML/refs/heads/main/film/movies.csv')\n",
        "movies.name = 'Movies'\n",
        "ratings = pl.read_csv(r'https://raw.githubusercontent.com/BogdanSliskovic/ML/refs/heads/main/film/ratings_RZS.csv')\n",
        "ratings.name = 'Ratings'\n",
        "\n",
        "for df in [movies, ratings]:\n",
        "  print(df.name , df.schema, df.shape)\n",
        "\n",
        "user, movies_feat, df = prep_pipeline(ratings, movies)\n",
        "X_user, X_movie, y, scalers = scale(df, user, movies_feat)\n",
        "\n",
        "def prep_tf(user, movies, training_batch = 16):\n",
        "  user, movies_feat, df = prep_pipeline(ratings, movies)\n",
        "  X_user, X_movie, y, scalers = scale(df, user, movies_feat)\n",
        "  data = (X_user, X_movie), y\n",
        "  data = tf.data.Dataset.from_tensor_slices(data).batch(training_batch)\n",
        "  return data\n",
        "###\n",
        "def split(data):\n",
        "  (X_user_test, X_movie_test), y_test = next(iter(data))\n",
        "  (X_user_dev, X_movie_dev), y_dev = next(iter(data.skip(1)))\n",
        "  train_data = data.skip(2).prefetch(tf.data.AUTOTUNE).repeat()\n",
        "  return ((X_user_test, X_movie_test, y_test), (X_user_dev, X_movie_dev, y_dev), train_data)\n",
        "\n",
        "data = prep_tf(ratings, movies)\n",
        "test_set, dev_set, train_data = split(data)\n",
        "\n",
        "X_user_test, X_movie_test, y_test = test_set\n",
        "X_user_dev, X_movie_dev, y_dev = dev_set\n",
        "\n",
        "\n",
        "model = ColaborativeFiltering(20, 23 ,user_layers = [256, 128, 64],embedding=64, learning_rate=0.001)#, user_reg = [regularizers.l2(0.01), None, None])\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True), ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)]\n",
        "history = model.fit(train_data, epochs=50,validation_data = ((X_user_dev, X_movie_dev), y_dev), callbacks=callbacks, steps_per_epoch = int(10000/16))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVRbhS_ZfROP",
        "outputId": "ae446cf5-6011-4bf2-c566-73b641e47edf"
      },
      "id": "lVRbhS_ZfROP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movies Schema([('movieid', Int64), ('title', String), ('genres', String)]) (87585, 3)\n",
            "Ratings Schema([('userid', Int64), ('movieid', Int64), ('rating', Float64)]) (10000, 3)\n",
            "Epoch 1/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0773 - mae: 0.2111 - mse: 0.0773 - val_loss: 0.0345 - val_mae: 0.1080 - val_mse: 0.0345 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0288 - mae: 0.1263 - mse: 0.0288 - val_loss: 0.0335 - val_mae: 0.1037 - val_mse: 0.0335 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0217 - mae: 0.1061 - mse: 0.0217 - val_loss: 0.0319 - val_mae: 0.1010 - val_mse: 0.0319 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0173 - mae: 0.0920 - mse: 0.0173 - val_loss: 0.0347 - val_mae: 0.1002 - val_mse: 0.0347 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m610/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0145 - mae: 0.0824 - mse: 0.0145\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0145 - mae: 0.0824 - mse: 0.0145 - val_loss: 0.0360 - val_mae: 0.0996 - val_mse: 0.0360 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0112 - mae: 0.0681 - mse: 0.0112 - val_loss: 0.0317 - val_mae: 0.0828 - val_mse: 0.0317 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - mae: 0.0641 - mse: 0.0101 - val_loss: 0.0314 - val_mae: 0.0814 - val_mse: 0.0314 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - mae: 0.0629 - mse: 0.0097 - val_loss: 0.0309 - val_mae: 0.0805 - val_mse: 0.0309 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0092 - mae: 0.0611 - mse: 0.0092 - val_loss: 0.0306 - val_mae: 0.0777 - val_mse: 0.0306 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0088 - mae: 0.0593 - mse: 0.0088 - val_loss: 0.0299 - val_mae: 0.0736 - val_mse: 0.0299 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0081 - mae: 0.0570 - mse: 0.0081 - val_loss: 0.0295 - val_mae: 0.0739 - val_mse: 0.0295 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0078 - mae: 0.0558 - mse: 0.0078 - val_loss: 0.0296 - val_mae: 0.0768 - val_mse: 0.0296 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0073 - mae: 0.0537 - mse: 0.0073 - val_loss: 0.0293 - val_mae: 0.0764 - val_mse: 0.0293 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0068 - mae: 0.0519 - mse: 0.0068 - val_loss: 0.0294 - val_mae: 0.0750 - val_mse: 0.0294 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0065 - mae: 0.0506 - mse: 0.0065 - val_loss: 0.0292 - val_mae: 0.0739 - val_mse: 0.0292 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - mae: 0.0494 - mse: 0.0062 - val_loss: 0.0289 - val_mae: 0.0721 - val_mse: 0.0289 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0059 - mae: 0.0481 - mse: 0.0059 - val_loss: 0.0305 - val_mae: 0.0774 - val_mse: 0.0305 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - mae: 0.0472 - mse: 0.0056 - val_loss: 0.0281 - val_mae: 0.0755 - val_mse: 0.0281 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - mae: 0.0468 - mse: 0.0056 - val_loss: 0.0295 - val_mae: 0.0780 - val_mse: 0.0295 - learning_rate: 5.0000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0053 - mae: 0.0457 - mse: 0.0053 - val_loss: 0.0280 - val_mae: 0.0753 - val_mse: 0.0280 - learning_rate: 5.0000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0051 - mae: 0.0448 - mse: 0.0051 - val_loss: 0.0287 - val_mae: 0.0782 - val_mse: 0.0287 - learning_rate: 5.0000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0050 - mae: 0.0440 - mse: 0.0050 - val_loss: 0.0272 - val_mae: 0.0705 - val_mse: 0.0272 - learning_rate: 5.0000e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0049 - mae: 0.0436 - mse: 0.0049 - val_loss: 0.0272 - val_mae: 0.0686 - val_mse: 0.0272 - learning_rate: 5.0000e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0050 - mae: 0.0439 - mse: 0.0050 - val_loss: 0.0269 - val_mae: 0.0706 - val_mse: 0.0269 - learning_rate: 5.0000e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0047 - mae: 0.0428 - mse: 0.0047 - val_loss: 0.0264 - val_mae: 0.0740 - val_mse: 0.0264 - learning_rate: 5.0000e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0044 - mae: 0.0415 - mse: 0.0044 - val_loss: 0.0270 - val_mae: 0.0731 - val_mse: 0.0270 - learning_rate: 5.0000e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m605/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0042 - mae: 0.0410 - mse: 0.0042\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0042 - mae: 0.0410 - mse: 0.0042 - val_loss: 0.0266 - val_mae: 0.0714 - val_mse: 0.0266 - learning_rate: 5.0000e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0038 - mae: 0.0369 - mse: 0.0038 - val_loss: 0.0254 - val_mae: 0.0620 - val_mse: 0.0254 - learning_rate: 2.5000e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0034 - mae: 0.0343 - mse: 0.0034 - val_loss: 0.0253 - val_mae: 0.0635 - val_mse: 0.0253 - learning_rate: 2.5000e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m614/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0033 - mae: 0.0337 - mse: 0.0033\n",
            "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - mae: 0.0337 - mse: 0.0033 - val_loss: 0.0255 - val_mae: 0.0657 - val_mse: 0.0255 - learning_rate: 2.5000e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0032 - mae: 0.0321 - mse: 0.0032 - val_loss: 0.0246 - val_mae: 0.0595 - val_mse: 0.0246 - learning_rate: 1.2500e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0030 - mae: 0.0309 - mse: 0.0030 - val_loss: 0.0246 - val_mae: 0.0587 - val_mse: 0.0246 - learning_rate: 1.2500e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m616/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0303 - mse: 0.0029\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0029 - mae: 0.0303 - mse: 0.0029 - val_loss: 0.0247 - val_mae: 0.0588 - val_mse: 0.0247 - learning_rate: 1.2500e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0028 - mae: 0.0291 - mse: 0.0028 - val_loss: 0.0243 - val_mae: 0.0577 - val_mse: 0.0243 - learning_rate: 6.2500e-05\n",
            "Epoch 35/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0283 - mse: 0.0027 - val_loss: 0.0242 - val_mae: 0.0579 - val_mse: 0.0242 - learning_rate: 6.2500e-05\n",
            "Epoch 36/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0283 - mse: 0.0027 - val_loss: 0.0242 - val_mae: 0.0585 - val_mse: 0.0242 - learning_rate: 6.2500e-05\n",
            "Epoch 37/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0282 - mse: 0.0027 - val_loss: 0.0242 - val_mae: 0.0592 - val_mse: 0.0242 - learning_rate: 6.2500e-05\n",
            "Epoch 38/50\n",
            "\u001b[1m598/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0279 - mse: 0.0027\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0279 - mse: 0.0027 - val_loss: 0.0242 - val_mae: 0.0598 - val_mse: 0.0242 - learning_rate: 6.2500e-05\n",
            "Epoch 39/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0273 - mse: 0.0027 - val_loss: 0.0241 - val_mae: 0.0589 - val_mse: 0.0241 - learning_rate: 3.1250e-05\n",
            "Epoch 40/50\n",
            "\u001b[1m619/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0272 - mse: 0.0027\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0272 - mse: 0.0027 - val_loss: 0.0241 - val_mae: 0.0590 - val_mse: 0.0241 - learning_rate: 3.1250e-05\n",
            "Epoch 41/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0267 - mse: 0.0026 - val_loss: 0.0241 - val_mae: 0.0587 - val_mse: 0.0241 - learning_rate: 1.5625e-05\n",
            "Epoch 42/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0265 - mse: 0.0026 - val_loss: 0.0241 - val_mae: 0.0587 - val_mse: 0.0241 - learning_rate: 1.5625e-05\n",
            "Epoch 43/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0262 - mse: 0.0026 - val_loss: 0.0240 - val_mae: 0.0586 - val_mse: 0.0240 - learning_rate: 1.5625e-05\n",
            "Epoch 44/50\n",
            "\u001b[1m603/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0265 - mse: 0.0026\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0265 - mse: 0.0026 - val_loss: 0.0240 - val_mae: 0.0587 - val_mse: 0.0240 - learning_rate: 1.5625e-05\n",
            "Epoch 45/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0262 - mse: 0.0026 - val_loss: 0.0240 - val_mae: 0.0584 - val_mse: 0.0240 - learning_rate: 7.8125e-06\n",
            "Epoch 46/50\n",
            "\u001b[1m598/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0260 - mse: 0.0026\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0260 - mse: 0.0026 - val_loss: 0.0240 - val_mae: 0.0584 - val_mse: 0.0240 - learning_rate: 7.8125e-06\n",
            "Epoch 47/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0027 - mae: 0.0261 - mse: 0.0027 - val_loss: 0.0239 - val_mae: 0.0584 - val_mse: 0.0239 - learning_rate: 3.9063e-06\n",
            "Epoch 48/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0259 - mse: 0.0026 - val_loss: 0.0239 - val_mae: 0.0583 - val_mse: 0.0239 - learning_rate: 3.9063e-06\n",
            "Epoch 49/50\n",
            "\u001b[1m614/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0259 - mse: 0.0026\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0259 - mse: 0.0026 - val_loss: 0.0239 - val_mae: 0.0582 - val_mse: 0.0239 - learning_rate: 3.9063e-06\n",
            "Epoch 50/50\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026 - mae: 0.0259 - mse: 0.0026 - val_loss: 0.0239 - val_mae: 0.0582 - val_mse: 0.0239 - learning_rate: 1.9531e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### inference.py\n",
        "m_net = model.movie_net\n",
        "m_embed = m_net.predict(X_movie)\n",
        "print(m_embed.shape)\n",
        "\n",
        "X_user_id, _, y_id, _ = scale(df, user, movies_feat, user_id = 28)\n",
        "u_id = X_user_id[0,0]\n",
        "m_id =\n",
        "X_user_id = X_user_id[0,1:]  ##SVAKI RED JE ISTI, A PRVA KOL JE USER_ID\n",
        "\n",
        "u_net = model.user_net\n",
        "u_embed = u_net.predict(tf.expand_dims(X_user_id,0))\n",
        "print(u_embed.shape)\n",
        "\n",
        "pred = tf.linalg.matmul(u_embed, m_embed, transpose_b= True)\n",
        "val, idx = tf.math.top_k(pred, k = 10)\n",
        "tf.gather(pred, idx, axis = 1) == val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BJEhBrgrPoS",
        "outputId": "ea6a1e6b-ba23-43ab-bd3f-6bbb7d4618fd"
      },
      "id": "5BJEhBrgrPoS",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "(10000, 64)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "(1, 64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 10), dtype=bool, numpy=\n",
              "array([[[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True]]])>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "4PeKpgUAVVLc",
        "outputId": "7053b89a-693f-4543-e54e-87ca2746396d"
      },
      "id": "4PeKpgUAVVLc",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10_000, 3)\n",
              "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
              "‚îÇ userid ‚îÜ movieid ‚îÜ rating ‚îÇ\n",
              "‚îÇ ---    ‚îÜ ---     ‚îÜ ---    ‚îÇ\n",
              "‚îÇ i64    ‚îÜ i64     ‚îÜ f64    ‚îÇ\n",
              "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
              "‚îÇ 36683  ‚îÜ 3741    ‚îÜ 4.0    ‚îÇ\n",
              "‚îÇ 16712  ‚îÜ 4082    ‚îÜ 2.5    ‚îÇ\n",
              "‚îÇ 131952 ‚îÜ 116161  ‚îÜ 2.5    ‚îÇ\n",
              "‚îÇ 147475 ‚îÜ 8360    ‚îÜ 4.0    ‚îÇ\n",
              "‚îÇ 131743 ‚îÜ 42004   ‚îÜ 4.0    ‚îÇ\n",
              "‚îÇ ‚Ä¶      ‚îÜ ‚Ä¶       ‚îÜ ‚Ä¶      ‚îÇ\n",
              "‚îÇ 40967  ‚îÜ 1007    ‚îÜ 3.5    ‚îÇ\n",
              "‚îÇ 16508  ‚îÜ 196     ‚îÜ 3.0    ‚îÇ\n",
              "‚îÇ 152948 ‚îÜ 78266   ‚îÜ 3.5    ‚îÇ\n",
              "‚îÇ 144987 ‚îÜ 182529  ‚îÜ 3.0    ‚îÇ\n",
              "‚îÇ 35682  ‚îÜ 143385  ‚îÜ 5.0    ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10_000, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>userid</th><th>movieid</th><th>rating</th></tr><tr><td>i64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>36683</td><td>3741</td><td>4.0</td></tr><tr><td>16712</td><td>4082</td><td>2.5</td></tr><tr><td>131952</td><td>116161</td><td>2.5</td></tr><tr><td>147475</td><td>8360</td><td>4.0</td></tr><tr><td>131743</td><td>42004</td><td>4.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>40967</td><td>1007</td><td>3.5</td></tr><tr><td>16508</td><td>196</td><td>3.0</td></tr><tr><td>152948</td><td>78266</td><td>3.5</td></tr><tr><td>144987</td><td>182529</td><td>3.0</td></tr><tr><td>35682</td><td>143385</td><td>5.0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "PA1ykl3JVZ-l",
        "outputId": "ff4b73ef-046e-4b82-d9f1-f8b4226282c0"
      },
      "id": "PA1ykl3JVZ-l",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (87_585, 3)\n",
              "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
              "‚îÇ movieid ‚îÜ title                           ‚îÜ genres                          ‚îÇ\n",
              "‚îÇ ---     ‚îÜ ---                             ‚îÜ ---                             ‚îÇ\n",
              "‚îÇ i64     ‚îÜ str                             ‚îÜ str                             ‚îÇ\n",
              "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
              "‚îÇ 1       ‚îÜ Toy Story (1995)                ‚îÜ Adventure|Animation|Children|C‚Ä¶ ‚îÇ\n",
              "‚îÇ 2       ‚îÜ Jumanji (1995)                  ‚îÜ Adventure|Children|Fantasy      ‚îÇ\n",
              "‚îÇ 3       ‚îÜ Grumpier Old Men (1995)         ‚îÜ Comedy|Romance                  ‚îÇ\n",
              "‚îÇ 4       ‚îÜ Waiting to Exhale (1995)        ‚îÜ Comedy|Drama|Romance            ‚îÇ\n",
              "‚îÇ 5       ‚îÜ Father of the Bride Part II (1‚Ä¶ ‚îÜ Comedy                          ‚îÇ\n",
              "‚îÇ ‚Ä¶       ‚îÜ ‚Ä¶                               ‚îÜ ‚Ä¶                               ‚îÇ\n",
              "‚îÇ 292731  ‚îÜ The Monroy Affaire (2022)       ‚îÜ Drama                           ‚îÇ\n",
              "‚îÇ 292737  ‚îÜ Shelter in Solitude (2023)      ‚îÜ Comedy|Drama                    ‚îÇ\n",
              "‚îÇ 292753  ‚îÜ Orca (2023)                     ‚îÜ Drama                           ‚îÇ\n",
              "‚îÇ 292755  ‚îÜ The Angry Breed (1968)          ‚îÜ Drama                           ‚îÇ\n",
              "‚îÇ 292757  ‚îÜ Race to the Summit (2023)       ‚îÜ Action|Adventure|Documentary    ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (87_585, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movieid</th><th>title</th><th>genres</th></tr><tr><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;Toy Story (1995)&quot;</td><td>&quot;Adventure|Animation|Children|C‚Ä¶</td></tr><tr><td>2</td><td>&quot;Jumanji (1995)&quot;</td><td>&quot;Adventure|Children|Fantasy&quot;</td></tr><tr><td>3</td><td>&quot;Grumpier Old Men (1995)&quot;</td><td>&quot;Comedy|Romance&quot;</td></tr><tr><td>4</td><td>&quot;Waiting to Exhale (1995)&quot;</td><td>&quot;Comedy|Drama|Romance&quot;</td></tr><tr><td>5</td><td>&quot;Father of the Bride Part II (1‚Ä¶</td><td>&quot;Comedy&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>292731</td><td>&quot;The Monroy Affaire (2022)&quot;</td><td>&quot;Drama&quot;</td></tr><tr><td>292737</td><td>&quot;Shelter in Solitude (2023)&quot;</td><td>&quot;Comedy|Drama&quot;</td></tr><tr><td>292753</td><td>&quot;Orca (2023)&quot;</td><td>&quot;Drama&quot;</td></tr><tr><td>292755</td><td>&quot;The Angry Breed (1968)&quot;</td><td>&quot;Drama&quot;</td></tr><tr><td>292757</td><td>&quot;Race to the Summit (2023)&quot;</td><td>&quot;Action|Adventure|Documentary&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCXIXllNR_9I"
      },
      "id": "wCXIXllNR_9I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}